# 大纲

- 中文分词的概念与分类

- 常用分词（规则分词、统计分词、混合分词）技术介绍

- 开源中文分词工具-Jieba

- 实战分词之高频词提取

---

# 3.1 中文分词简介

1. 规则分词

	最早兴起，主要通过人工设立词库，按照一定方式进行匹配切分，实现简单高效，但对新词难以处理；

2. 统计分词
	
	能较好应对新词发现场景，但是太过于依赖于语料质量；

3. 混合分词

	规则分词与统计分词的结合体；

---

# 3.2 规则分词

- 定义

	一种机械分词方法，主要通过维护词典，切分语句时，将语句中的每个字符串与词表中的词逐一匹配，找到则切分，否则不切分；

- 分类

	- 正向最大匹配法（Maximum Match Method，MM法）

	1. 基本思想

		假定分词词典中最长词有$i$个汉字字符，则用被处理文档的当前字符串中的前$i$个字作为匹配字段，查找字典；

	2. 算法描述

		- 从左向右取待切分汉语句的$m$个字符作为匹配字段，$m$是机器词典中最长词条的字符数；
		- 查找机器词典并进行匹配。匹配成功则将匹配字段作为一个词切分出来，匹配失败则将匹配字段的最后一个字去掉，剩下的字符串作为新的匹配字段，进行再匹配，一直重复上述过程指导切分出所有词；

	- 逆向最大匹配法（Reverse Maximum Match Method，RMM法）

	1. 基本原理

		从被处理文档末端开始匹配扫描，每次取末端的$i$个字符作为匹配字段，匹配事变则去掉匹配字段最前一个字，继续匹配；

	- 双向最大匹配(Bi-direction Matching Method)

	1. 基本原理
		
		将正向最大匹配法和逆向最大匹配法得到的分词结果进行比较，庵后按照最大匹配原则，选取词数切分最少的作为结果；

- 相关代码

1. 正向最大匹配

```python
	
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @version : 1.0
# @Time    : 2019-8-25 15:27
# @Author  : cunyu
# @Email   : cunyu1024@foxmail.com
# @Site    : https://cunyu1943.github.io
# @File    : mm.py
# @Software: PyCharm
# @Desc    : 正向最大匹配分词

train_data = './data/train.txt'  # 训练语料
test_data = './data/test.txt'  # 测试语料
result_data = './data/test_sc_zhengxiang.txt'  # 生成结果


def get_dic(train_data):  # 读取文本返回列表
    with open(train_data, 'r', encoding='utf-8', ) as f:
        try:
            file_content = f.read().split()
        finally:
            f.close()
    chars = list(set(file_content))
    return chars


def MM(test_data, result_data, dic):
    # 词的最大长度
    max_length = 5

    h = open(result_data, 'w', encoding='utf-8', )
    with open(test_data, 'r', encoding='utf-8', ) as f:
        lines = f.readlines()

    for line in lines:  # 分别对每行进行正向最大匹配处理
        max_length = 5
        my_list = []
        len_hang = len(line)
        while len_hang > 0:
            tryWord = line[0:max_length]
            while tryWord not in dic:
                if len(tryWord) == 1:
                    break
                tryWord = tryWord[0:len(tryWord) - 1]
            my_list.append(tryWord)
            line = line[len(tryWord):]
            len_hang = len(line)

        for t in my_list:  # 将分词结果写入生成文件
            if t == '\n':
                h.write('\n')
            else:
                print(t)
                h.write(t + "  ")
    h.close()


if __name__ == '__main__':
    print('读入词典')
    dic = get_dic(train_data)
    print('开始匹配')
    MM(test_data, result_data, dic)

```

2. 逆向最大匹配

```python

#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @version : 1.0
# @Time    : 2019-8-25 15:36
# @Author  : cunyu
# @Email   : cunyu1024@foxmail.com
# @Site    : https://cunyu1943.github.io
# @File    : rmm.py
# @Software: PyCharm
# @Desc    : 逆向最大匹配法

train_data = './data/train.txt'
test_data = './data/test.txt'
result_data = './data/test_sc.txt'


def get_dic(train_data):
    with open(train_data, 'r', encoding='utf-8', ) as f:
        try:
            file_content = f.read().split()
        finally:
            f.close()
    chars = list(set(file_content))
    return chars

def RMM(test_data,result_data,dic):
    max_length = 5

    h = open(result_data, 'w', encoding='utf-8', )
    with open(test_data, 'r', encoding='utf-8', ) as f:
        lines = f.readlines()

    for line in lines:
        my_stack = []
        len_hang = len(line)
        while len_hang > 0:
            tryWord = line[-max_length:]
            while tryWord not in dic:
                if len(tryWord) == 1:
                    break
                tryWord = tryWord[1:]
            my_stack.append(tryWord)
            line = line[0:len(line) - len(tryWord)]
            len_hang = len(line)

        while len(my_stack):
            t = my_stack.pop()
            if t == '\n':
                h.write('\n')
            else:
                print(t)
                h.write(t + "  ")

    h.close()


if __name__ == '__main__':
    print('获取字典')
    dic = get_dic(train_data)
    print('开始匹配……')
    RMM(test_data,result_data,dic)
```

---

# 3.3 统计分词

- 主要操作

	- 建立统计语言模型；
	- 对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大额分词方式，常用统计学习算法有隐含马尔可夫（HMM）、条件随机场（CRF）等；

- n元条件概率

$$P(w_i|w_{i-(n-1)},…,w_{i-1})=\frac{count(w_{i-(n-1)},…,w_{i-1},w_i)}{count(w_{i-(n-1)},…,w_{i-1})}$$

其中，$count(w_{i-(n-1)},…,w_{i-1})$表示词$w_{i-(n-1)},…,w_{i-1}$在语料库中出现的总次数；

- HMM模型

	将分词作为字在字串中的序列标注任务来实现。基本思路：每个字在构造一个特定的词语时都占据着一个确定的构词位置（即词位），规定每个字最多有四个构词位置：B（词首）、M（词中）、E（词尾）、S（单独成词）；

	- 数学理论

		假设用$\lambda=\lambda _1 \lambda_2 …\lambda_n$代表输入的句子，$n$表示句子长度，$\lambda_i$表示字，$o=o_1o_2…o_n$表示输出的标签，则理想输出为：
$$max = maxP(o=o_1o_2…o_n|\lambda=\lambda _1 \lambda_2 …\lambda_n)=P(o_1|\lambda_1)P(o_2|\lambda_2)…P(o_n|\lambda_n)$$

在这个算法中，求解结果的常用方法是Veterbi算法，这是一种动态规划算法，**核心**是：如果最终的最优路径经过某个节点$o_i$，则从初始节点到$o_{i-1}$点的路径必然也是一个最优路径，因为每个节点$o_i$只会影响前后两个$$和$P()$；

- 其他统计分词算法

	- 条件随机场（CRF）
	- 神经网络分词算法（CNN、LSTM）

---

# 3.5 中文分词工具——Jieba