# 大纲

- 文本向量化常用算法介绍，word2vec及doc2vec
- 向量化方法的模型训练和使用

---

# 7.1 文本向量化概述

即将文本表示为一系列能表达文本语义的向量；

---

# 7.2 向量化算法word2vec

- 词袋（Bag of Word）模型：最早的以词语为基本处理单元的文本向量化方法；

- 词袋模型存在的问题：

	- 维度灾难
	- 无法保留词序信息
	- 存在语义鸿沟问题

- 神经网络语言模型（NNLM）

	![NNLM语言模型](https://i.loli.net/2019/08/30/16CTtBQzNcGg9Ek.png)

	大致操作步骤：从语料库中收集一系列长度为$n$的文本序列$w_{i-(n-1)},…,w_{i-1},w_i$，设这个长度为$n$的文本序列组成的集合为$D$，则NNML的目标函数定义为：

	$$\sum _DP(w_i|w_{i-(n-1)},…,w_{i-1})$$

	即：在输入词序列为$w_{i-(n-1)},…,w_{i-1}$的情况下，计算目标词$w_i$的概率；

	在上述经典三层前馈神经网络结构中：为解决词袋模型数据稀疏问题，输入层的输入为低纬度的、紧密的词向量，将词序列$w_{i-(n-1)},…,w_{i-1}$中每个词向量按顺序进行拼接，即：

	$$x=[v(w_{i-(n-1)});…;v(w_{i-2});v(w_{i-1})]$$

	接下来，$x$经隐藏层得到$h$，再将$h$接入输入层从而得到最后的输出变量$y$，其中：

	$$h=tanh(b+Hx)$$

	$$y=b+Uh$$

	其中$H$是输入层到隐藏层的权重矩阵，维度为$|h|\times(n-1)|e|$；$U$是隐藏层到输出层的权重矩阵，维度为$|V|\times |h|$，其中$|V|$表示词表大小，$b$则是模型中的偏置项；为保证输出$y(w)$的表示概率值，需对输出层进行归一化操作，一般是加入$softmax$码数，将$y$转化成对应概率值：

	$$P(w_i|w_{i-(n-1)}…,w_{i-1})=\frac {exp(y(w_i))}{\sum _{k=1}^{|V|}exp(y(w_k))}$$

	因为输出是在上下文词序列出现的情况下，下一个词的概率，所以语料库$D$中最大化$y(w_i)$即为$NNLM$模型的目标函数：

	$$\sum _{w_{i-(n-1);i in D}}log P(w_i|w_{i-(n-1)},…,w_{i-1})$$

	一般使用随机梯度下降法对模型进行训练，对于每个$batch$，随机从语料库中抽取若干样本进行训练，其迭代公式为：

	$$\theta:\theta+\alpha \frac{\partial logP(w_i|w_{i-(n-1)},…,w_{i-1})}{\partial \theta}$$

	$\alpha$$$