# 大纲

- 文本向量化常用算法介绍，word2vec及doc2vec
- 向量化方法的模型训练和使用

---

# 7.1 文本向量化概述

即将文本表示为一系列能表达文本语义的向量；

---

# 7.2 向量化算法word2vec

- 词袋（Bag of Word）模型：最早的以词语为基本处理单元的文本向量化方法；

- 词袋模型存在的问题：

	- 维度灾难
	- 无法保留词序信息
	- 存在语义鸿沟问题

- 神经网络语言模型（NNLM）

	![NNLM语言模型](https://i.loli.net/2019/08/30/16CTtBQzNcGg9Ek.png)

	大致操作步骤：从语料库中收集一系列长度为$n$的文本序列$w_{i-(n-1)},…,w_{i-1},w_i$，设这个长度为$n$的文本序列组成的集合为$D$，则NNML的目标函数定义为：

	$$\sum _DP(w_i|w_{i-(n-1)},…,w_{i-1})$$

	即：在输入词序列为$w_{i-(n-1)},…,w_{i-1}$的情况下，计算目标词$w_i$的概率；

	在上述经典三层前馈神经网络结构中：为解决词袋模型数据稀疏问题，输入层的输入为低纬度的、紧密的词向量，将词序列$w_{i-(n-1)},…,w_{i-1}$中每个词向量按顺序进行拼接，即：

	$$x=[v(w_{i-(n-1)});…;v(w_{i-2});v(w_{i-1})]$$

	接下来，$x$经隐藏层得到$h$，再将$h$接入输入层从而得到最后的输出变量$y$，其中：

	$$h=tanh(b+Hx)$$

	$$y=b+Uh$$

	其中$H$是输入层到隐藏层的权重矩阵，维度为$|h|\times(n-1)|e|$；$U$是隐藏层到输出层的权重矩阵，维度为$|V|\times |h|$，其中$|V|$表示词表大小，$b$则是模型中的偏置项；为保证输出$y(w)$的表示概率值，需对输出层进行归一化操作，一般是加入$softmax$码数，将$y$转化成对应概率值：

	$$P(w_i|w_{i-(n-1)}…,w_{i-1})=\frac {exp(y(w_i))}{\sum _{k=1}^{|V|}exp(y(w_k))}$$

	因为输出是在上下文词序列出现的情况下，下一个词的概率，所以语料库$D$中最大化$y(w_i)$即为$NNLM$模型的目标函数：

	$$\sum _{w_{i-(n-1);i in D}}log P(w_i|w_{i-(n-1)},…,w_{i-1})$$

	一般使用随机梯度下降法对模型进行训练，对于每个$batch$，随机从语料库中抽取若干样本进行训练，其迭代公式为：

	$$\theta:\theta+\alpha \frac{\partial logP(w_i|w_{i-(n-1)},…,w_{i-1})}{\partial \theta}$$

	$\alpha$为学习率，$\theta$包括模型中设计所有参数，包括$NNLM$模型中的权重、偏置以及输入词向量；

- C&W模型

	![C&W模型结构图](https://i.loli.net/2019/08/31/V12SxoGnalDNwke.png)

	$C&W$模型未采用语言模型的方式求解词语上下文的条件概率，而是直接对$n$元短语打分，其核心机理为：若$n$元短语在语料中出现过，则模型给该短语打一个高分，若未出现的短语，则赋予一个较低评分，其目标函数为：

	$$\sum _{(w,c)\in D} \sum_{w^` \in V}max(0,1-score(w,c)+score(w^`,c))$$

	$(w,c)$是从语料中抽取的$n$元短语，为保证上下文词数一致性$n$应该为奇数，$w$是目标词，$c$表示目标词上下文语境，$w^`$是词典中随机抽取的词；

- CBOW模型

	![CBOW模型结构图](https://i.loli.net/2019/08/31/iyZ941D68EKNgrU.png)

	根据上下文来预测当前词语的概率，使用文本中间词作为目标词，同时去除隐藏层从而提升运算速率，其输入层激吻语义上下文的表示；

	对目标词的条件概率公式如下：

	$$P(w|c)=\frac {exp(e^`(w)^Tx)}{\sum _{w^` \in V} exp(e^`(w^`)^Tx)}$$

	其目标函数类似于$NNLM$模型，为最大化式：

	$$\sum _{(w,c)\in D}logP(w,c)$$

- Skim-gram模型
	
	![SG模型结构图](https://i.loli.net/2019/08/31/UNFVnfweLiOtHI5.png)

	根据当前词语来预测上下文概率，从目标词$w$的上下文中选择一个词，然后将其词向量组成上下文的表示，模型目标函数为：

	$$max(\sum_{(w,c)\in D} \sum _{w_j \in c}log P(w|w_j))$$

---

# 7.3 向量化算法doc2vec/str2vec

	doc2vec包括DM和DBOW（）